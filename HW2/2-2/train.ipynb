{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common packages\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# # DL framework\n",
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "from attractivedata import AttractiveData\n",
    "from trainer import AttractiveTrainer"
   ]
  },
  {
   "source": [
    "# TODO: \n",
    "- [ ] try to use only second layer's hidden state\n",
    "- [ ] word embedding multiply tfidf\n",
    "- [ ] use word2vec\n",
    "- [ ] k-fold cv"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'data/train.csv'\n",
    "val_file = 'example/val.csv'\n",
    "test_file = 'data/test.csv'\n",
    "pretrained_file = 'glove.840B.300d'\n",
    "config = {\n",
    "    'max_seq': 40,\n",
    "    'min_freq': 0,\n",
    "    'batch_size': 64,\n",
    "    'pretrained_file': pretrained_file,\n",
    "    'n_splits': 10\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AttractiveData = AttractiveData(train_file, val_file, test_file, pretrained_file, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "fold_data = [[each_train, each_val] for each_train, each_val in AttractiveData.get_data()]\n",
    "len(fold_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, sentence in enumerate(AttractiveData.test_data):\n",
    "#     if i == 3:\n",
    "#         print(vars(AttractiveData.train_data[i]), vars(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "max_len = 0\n",
    "a = AttractiveData.train_data\n",
    "for i in range(len(a)):\n",
    "    if len(a[i].Headline) >= max_len:\n",
    "        max_len = len(a[i].Headline)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([12699, 300])\n"
     ]
    }
   ],
   "source": [
    "config['timestr'] = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "config['save_name'] = 'CNN_LSTM'\n",
    "config['input_dim'] = len(AttractiveData.TEXT.vocab)\n",
    "config['embedding_dim'] = 300\n",
    "config['category_dim'] = len(AttractiveData.CATEGORIES_LABEL.vocab)\n",
    "config['category_embedding_dim'] = 8\n",
    "config['hidden_dim'] = 30\n",
    "config['output_dim'] = 1\n",
    "config['log_steps'] = 10\n",
    "config['epochs'] = 100\n",
    "config['lr'] = {\n",
    "    'encoder': 1e-5,\n",
    "    'embedding': 6e-6,\n",
    "    'linear': 1e-5\n",
    "}\n",
    "config['num_layers'] = 1\n",
    "config['kernel_size'] = 3\n",
    "config['dropout'] = 0.1\n",
    "config['train_len'] = AttractiveData.train_len\n",
    "config['val_len'] = AttractiveData.val_len\n",
    "config['test_len'] = AttractiveData.test_len\n",
    "\n",
    "pretrained_embeddings = AttractiveData.TEXT.vocab.vectors\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "AttractiveTrainer = AttractiveTrainer(config, AttractiveData.device, fold_data, pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(AttractiveNet(\n",
       "   (embedding): AttractiveEmbedding(\n",
       "     (token): TokenEmbedding(12699, 300, padding_idx=1)\n",
       "   )\n",
       "   (bigramcnn): Sequential(\n",
       "     (0): Conv1d(300, 200, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "     (1): ReLU()\n",
       "     (2): Conv1d(200, 100, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (trigramcnn): Sequential(\n",
       "     (0): Conv1d(300, 200, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "     (1): ReLU()\n",
       "     (2): Conv1d(200, 100, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "     (3): ReLU()\n",
       "   )\n",
       "   (encoder_bigram_first): LSTM(100, 30, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "   (encoder_trigram_first): LSTM(100, 30, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "   (linear): Sequential(\n",
       "     (0): Linear(in_features=120, out_features=30, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=30, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " 4357321,\n",
       " 4357321)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "AttractiveTrainer.model, AttractiveTrainer.config['total_params'], AttractiveTrainer.config['total_learned_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch:   1%|          | 1/100 [00:03<05:56,  3.60s/it]\n",
      "EP_0 | 10-fold val loss: 0.7706972982369217 |\n",
      "Epoch:   2%|▏         | 2/100 [00:07<05:51,  3.59s/it]\n",
      "EP_1 | 10-fold val loss: 0.5390480269404019 |\n",
      "Epoch:   3%|▎         | 3/100 [00:10<05:49,  3.60s/it]\n",
      "EP_2 | 10-fold val loss: 0.507658905842725 |\n",
      "Epoch:   4%|▍         | 4/100 [00:14<05:46,  3.61s/it]\n",
      "EP_3 | 10-fold val loss: 0.47107828896419673 |\n",
      "Epoch:   5%|▌         | 5/100 [00:18<05:47,  3.66s/it]\n",
      "EP_4 | 10-fold val loss: 0.4402805333044014 |\n",
      "Epoch:   6%|▌         | 6/100 [00:21<05:47,  3.69s/it]\n",
      "EP_5 | 10-fold val loss: 0.4189519732606177 |\n",
      "Epoch:   7%|▋         | 7/100 [00:25<05:42,  3.68s/it]\n",
      "EP_6 | 10-fold val loss: 0.4007869742664636 |\n",
      "Epoch:   8%|▊         | 8/100 [00:29<05:38,  3.68s/it]\n",
      "EP_7 | 10-fold val loss: 0.38224240623268424 |\n",
      "Epoch:   9%|▉         | 9/100 [00:32<05:31,  3.65s/it]\n",
      "EP_8 | 10-fold val loss: 0.36132723694922875 |\n",
      "Epoch:  10%|█         | 10/100 [00:36<05:25,  3.61s/it]\n",
      "EP_9 | 10-fold val loss: 0.3364764873887979 |\n",
      "Epoch:  11%|█         | 11/100 [00:39<05:19,  3.59s/it]\n",
      "EP_10 | 10-fold val loss: 0.3058034223668716 |\n",
      "Epoch:  12%|█▏        | 12/100 [00:43<05:15,  3.59s/it]\n",
      "EP_11 | 10-fold val loss: 0.26873101463504867 |\n",
      "Epoch:  13%|█▎        | 13/100 [00:47<05:10,  3.57s/it]\n",
      "EP_12 | 10-fold val loss: 0.22648769605393504 |\n",
      "Epoch:  14%|█▍        | 14/100 [00:50<05:06,  3.56s/it]\n",
      "EP_13 | 10-fold val loss: 0.1804573372882955 |\n",
      "Epoch:  15%|█▌        | 15/100 [00:54<05:02,  3.56s/it]\n",
      "EP_14 | 10-fold val loss: 0.12890232351480746 |\n",
      "Epoch:  16%|█▌        | 16/100 [00:57<04:58,  3.56s/it]\n",
      "EP_15 | 10-fold val loss: 0.08607149639842557 |\n",
      "Epoch:  17%|█▋        | 17/100 [01:01<04:54,  3.55s/it]\n",
      "EP_16 | 10-fold val loss: 0.0480706625417167 |\n",
      "Epoch:  18%|█▊        | 18/100 [01:04<04:51,  3.56s/it]\n",
      "EP_17 | 10-fold val loss: 0.027315739293893176 |\n",
      "Epoch:  19%|█▉        | 19/100 [01:08<04:49,  3.58s/it]\n",
      "EP_18 | 10-fold val loss: 0.01670531681099651 |\n",
      "Epoch:  20%|██        | 20/100 [01:11<04:45,  3.57s/it]\n",
      "EP_19 | 10-fold val loss: 0.010263857048223999 |\n",
      "Epoch:  21%|██        | 21/100 [01:15<04:44,  3.61s/it]\n",
      "EP_20 | 10-fold val loss: 0.005980788142986449 |\n",
      "Epoch:  22%|██▏       | 22/100 [01:19<04:44,  3.64s/it]\n",
      "EP_21 | 10-fold val loss: 0.0036808857045994665 |\n",
      "Epoch:  23%|██▎       | 23/100 [01:23<04:44,  3.69s/it]\n",
      "EP_22 | 10-fold val loss: 0.002305740219376543 |\n",
      "Epoch:  24%|██▍       | 24/100 [01:26<04:38,  3.66s/it]\n",
      "EP_23 | 10-fold val loss: 0.001557823069229284 |\n",
      "Epoch:  25%|██▌       | 25/100 [01:30<04:34,  3.66s/it]\n",
      "EP_24 | 10-fold val loss: 0.0011301792265536884 |\n",
      "Epoch:  26%|██▌       | 26/100 [01:34<04:29,  3.64s/it]\n",
      "EP_25 | 10-fold val loss: 0.0008709760318857198 |\n",
      "Epoch:  27%|██▋       | 27/100 [01:37<04:26,  3.65s/it]\n",
      "EP_26 | 10-fold val loss: 0.0007003729730986022 |\n",
      "Epoch:  28%|██▊       | 28/100 [01:41<04:22,  3.64s/it]\n",
      "EP_27 | 10-fold val loss: 0.0005830871950214107 |\n",
      "Epoch:  29%|██▉       | 29/100 [01:45<04:22,  3.69s/it]\n",
      "EP_28 | 10-fold val loss: 0.00050317510493401 |\n",
      "Epoch:  30%|███       | 30/100 [01:48<04:15,  3.65s/it]\n",
      "EP_29 | 10-fold val loss: 0.00044464142730587397 |\n",
      "Epoch:  31%|███       | 31/100 [01:52<04:07,  3.58s/it]\n",
      "EP_30 | 10-fold val loss: 0.0004032376284229422 |\n",
      "Epoch:  32%|███▏      | 32/100 [01:55<04:02,  3.56s/it]\n",
      "EP_31 | 10-fold val loss: 0.0003710166568863297 |\n",
      "Epoch:  33%|███▎      | 33/100 [01:59<03:56,  3.53s/it]\n",
      "EP_32 | 10-fold val loss: 0.000343694053898456 |\n",
      "Epoch:  34%|███▍      | 34/100 [02:02<03:50,  3.50s/it]\n",
      "EP_33 | 10-fold val loss: 0.00032012031128853745 |\n",
      "Epoch:  35%|███▌      | 35/100 [02:05<03:46,  3.49s/it]\n",
      "EP_34 | 10-fold val loss: 0.00030169329821707355 |\n",
      "Epoch:  36%|███▌      | 36/100 [02:09<03:45,  3.53s/it]\n",
      "EP_35 | 10-fold val loss: 0.000286272601863625 |\n",
      "Epoch:  37%|███▋      | 37/100 [02:13<03:40,  3.49s/it]\n",
      "EP_36 | 10-fold val loss: 0.0002722434095773554 |\n",
      "Epoch:  38%|███▊      | 38/100 [02:16<03:38,  3.53s/it]\n",
      "EP_37 | 10-fold val loss: 0.0002599534243071342 |\n",
      "Epoch:  39%|███▉      | 39/100 [02:20<03:42,  3.64s/it]\n",
      "EP_38 | 10-fold val loss: 0.00024810754999349477 |\n",
      "Epoch:  40%|████      | 40/100 [02:24<03:36,  3.61s/it]\n",
      "EP_39 | 10-fold val loss: 0.0002380741990968019 |\n",
      "Epoch:  41%|████      | 41/100 [02:27<03:33,  3.62s/it]\n",
      "EP_40 | 10-fold val loss: 0.0002292515018922916 |\n",
      "Epoch:  42%|████▏     | 42/100 [02:31<03:33,  3.68s/it]\n",
      "EP_41 | 10-fold val loss: 0.0002211322640719385 |\n",
      "Epoch:  43%|████▎     | 43/100 [02:35<03:29,  3.67s/it]\n",
      "EP_42 | 10-fold val loss: 0.00021450810468819575 |\n",
      "Epoch:  44%|████▍     | 44/100 [02:38<03:28,  3.72s/it]\n",
      "EP_43 | 10-fold val loss: 0.0002078678359062977 |\n",
      "Epoch:  45%|████▌     | 45/100 [02:42<03:20,  3.64s/it]\n",
      "EP_44 | 10-fold val loss: 0.00020209329982297312 |\n",
      "Epoch:  46%|████▌     | 46/100 [02:45<03:12,  3.56s/it]\n",
      "EP_45 | 10-fold val loss: 0.0001973827377215083 |\n",
      "Epoch:  47%|████▋     | 47/100 [02:49<03:06,  3.51s/it]\n",
      "EP_46 | 10-fold val loss: 0.00019187381495059668 |\n",
      "Epoch:  48%|████▊     | 48/100 [02:52<03:00,  3.48s/it]\n",
      "EP_47 | 10-fold val loss: 0.0001862791829255145 |\n",
      "Epoch:  49%|████▉     | 49/100 [02:56<02:55,  3.45s/it]\n",
      "EP_48 | 10-fold val loss: 0.0001817173587601157 |\n",
      "Epoch:  50%|█████     | 50/100 [02:59<02:52,  3.45s/it]\n",
      "EP_49 | 10-fold val loss: 0.00017703039373052033 |\n",
      "Epoch:  51%|█████     | 51/100 [03:02<02:48,  3.43s/it]\n",
      "EP_50 | 10-fold val loss: 0.0001729795029592317 |\n",
      "Epoch:  52%|█████▏    | 52/100 [03:06<02:43,  3.41s/it]\n",
      "EP_51 | 10-fold val loss: 0.00016908787045965667 |\n",
      "Epoch:  53%|█████▎    | 53/100 [03:09<02:40,  3.41s/it]\n",
      "EP_52 | 10-fold val loss: 0.00016529980448289208 |\n",
      "Epoch:  54%|█████▍    | 54/100 [03:13<02:36,  3.40s/it]\n",
      "EP_53 | 10-fold val loss: 0.00016196004236881695 |\n",
      "Epoch:  55%|█████▌    | 55/100 [03:16<02:32,  3.39s/it]\n",
      "EP_54 | 10-fold val loss: 0.00015880714236831694 |\n",
      "Epoch:  56%|█████▌    | 56/100 [03:19<02:30,  3.42s/it]\n",
      "EP_55 | 10-fold val loss: 0.00015597014286208386 |\n",
      "Epoch:  57%|█████▋    | 57/100 [03:23<02:26,  3.41s/it]\n",
      "EP_56 | 10-fold val loss: 0.00015328347836461318 |\n",
      "Epoch:  58%|█████▊    | 58/100 [03:26<02:23,  3.42s/it]\n",
      "EP_57 | 10-fold val loss: 0.00015072446728356902 |\n",
      "Epoch:  59%|█████▉    | 59/100 [03:30<02:19,  3.41s/it]\n",
      "EP_58 | 10-fold val loss: 0.00014801236745067096 |\n",
      "Epoch:  60%|██████    | 60/100 [03:33<02:16,  3.41s/it]\n",
      "EP_59 | 10-fold val loss: 0.00014522709925407907 |\n",
      "Epoch:  61%|██████    | 61/100 [03:37<02:14,  3.44s/it]\n",
      "EP_60 | 10-fold val loss: 0.00014279866427743546 |\n",
      "Epoch:  62%|██████▏   | 62/100 [03:40<02:09,  3.40s/it]\n",
      "EP_61 | 10-fold val loss: 0.00014060036473001318 |\n",
      "Epoch:  63%|██████▎   | 63/100 [03:43<02:04,  3.36s/it]\n",
      "EP_62 | 10-fold val loss: 0.0001385443068797673 |\n",
      "Epoch:  64%|██████▍   | 64/100 [03:46<02:00,  3.35s/it]\n",
      "EP_63 | 10-fold val loss: 0.0001364553999355492 |\n",
      "Epoch:  65%|██████▌   | 65/100 [03:50<01:57,  3.37s/it]\n",
      "EP_64 | 10-fold val loss: 0.00013449726173740938 |\n",
      "Epoch:  66%|██████▌   | 66/100 [03:53<01:54,  3.37s/it]\n",
      "EP_65 | 10-fold val loss: 0.00013264110373212117 |\n",
      "Epoch:  67%|██████▋   | 67/100 [03:57<01:51,  3.38s/it]\n",
      "EP_66 | 10-fold val loss: 0.00013076892028752185 |\n",
      "Epoch:  68%|██████▊   | 68/100 [04:00<01:49,  3.42s/it]\n",
      "EP_67 | 10-fold val loss: 0.00012916049197562695 |\n",
      "Epoch:  69%|██████▉   | 69/100 [04:03<01:45,  3.40s/it]\n",
      "EP_68 | 10-fold val loss: 0.00012757402479359836 |\n",
      "Epoch:  70%|███████   | 70/100 [04:07<01:41,  3.39s/it]\n",
      "EP_69 | 10-fold val loss: 0.00012602414342337382 |\n",
      "Epoch:  71%|███████   | 71/100 [04:10<01:37,  3.35s/it]\n",
      "EP_70 | 10-fold val loss: 0.0001245572182204912 |\n",
      "Epoch:  72%|███████▏  | 72/100 [04:13<01:34,  3.36s/it]\n",
      "EP_71 | 10-fold val loss: 0.0001229823204459321 |\n",
      "Epoch:  73%|███████▎  | 73/100 [04:17<01:30,  3.35s/it]\n",
      "EP_72 | 10-fold val loss: 0.00012167405150773677 |\n",
      "Epoch:  74%|███████▍  | 74/100 [04:20<01:26,  3.34s/it]\n",
      "EP_73 | 10-fold val loss: 0.00012044044555759507 |\n",
      "Epoch:  75%|███████▌  | 75/100 [04:23<01:22,  3.31s/it]\n",
      "EP_74 | 10-fold val loss: 0.00011924763311981224 |\n",
      "Epoch:  76%|███████▌  | 76/100 [04:27<01:19,  3.29s/it]\n",
      "EP_75 | 10-fold val loss: 0.00011819543988345758 |\n",
      "Epoch:  77%|███████▋  | 77/100 [04:30<01:16,  3.31s/it]\n",
      "EP_76 | 10-fold val loss: 0.00011720323996354744 |\n",
      "Epoch:  78%|███████▊  | 78/100 [04:33<01:12,  3.30s/it]\n",
      "EP_77 | 10-fold val loss: 0.00011604676109723812 |\n",
      "Epoch:  79%|███████▉  | 79/100 [04:37<01:09,  3.30s/it]\n",
      "EP_78 | 10-fold val loss: 0.00011487151386259654 |\n",
      "Epoch:  80%|████████  | 80/100 [04:40<01:06,  3.30s/it]\n",
      "EP_79 | 10-fold val loss: 0.00011374338789441846 |\n",
      "Epoch:  81%|████████  | 81/100 [04:43<01:02,  3.30s/it]\n",
      "EP_80 | 10-fold val loss: 0.00011258085944385076 |\n",
      "Epoch:  82%|████████▏ | 82/100 [04:46<00:59,  3.29s/it]\n",
      "EP_81 | 10-fold val loss: 0.00011140776290381373 |\n",
      "Epoch:  83%|████████▎ | 83/100 [04:50<00:56,  3.29s/it]\n",
      "EP_82 | 10-fold val loss: 0.00011037863774867685 |\n",
      "Epoch:  84%|████████▍ | 84/100 [04:53<00:52,  3.30s/it]\n",
      "EP_83 | 10-fold val loss: 0.00010927918110432004 |\n",
      "Epoch:  85%|████████▌ | 85/100 [04:56<00:50,  3.35s/it]\n",
      "EP_84 | 10-fold val loss: 0.00010835828418873694 |\n",
      "Epoch:  86%|████████▌ | 86/100 [05:00<00:46,  3.34s/it]\n",
      "EP_85 | 10-fold val loss: 0.00010746213180633882 |\n",
      "Epoch:  87%|████████▋ | 87/100 [05:03<00:43,  3.32s/it]\n",
      "EP_86 | 10-fold val loss: 0.00010663900957653967 |\n",
      "Epoch:  88%|████████▊ | 88/100 [05:06<00:39,  3.32s/it]\n",
      "EP_87 | 10-fold val loss: 0.00010581918168991607 |\n",
      "Epoch:  89%|████████▉ | 89/100 [05:10<00:36,  3.31s/it]\n",
      "EP_88 | 10-fold val loss: 0.00010494460484016385 |\n",
      "Epoch:  90%|█████████ | 90/100 [05:13<00:32,  3.30s/it]\n",
      "EP_89 | 10-fold val loss: 0.00010416638722067458 |\n",
      "Epoch:  91%|█████████ | 91/100 [05:16<00:29,  3.29s/it]\n",
      "EP_90 | 10-fold val loss: 0.00010325588712779184 |\n",
      "Epoch:  92%|█████████▏| 92/100 [05:20<00:26,  3.30s/it]\n",
      "EP_91 | 10-fold val loss: 0.00010251931406160376 |\n",
      "Epoch:  93%|█████████▎| 93/100 [05:23<00:23,  3.29s/it]\n",
      "EP_92 | 10-fold val loss: 0.0001018417600147693 |\n",
      "Epoch:  94%|█████████▍| 94/100 [05:26<00:19,  3.32s/it]\n",
      "EP_93 | 10-fold val loss: 0.00010112382351508131 |\n",
      "Epoch:  95%|█████████▌| 95/100 [05:30<00:16,  3.34s/it]\n",
      "EP_94 | 10-fold val loss: 0.00010041419837387579 |\n",
      "Epoch:  96%|█████████▌| 96/100 [05:33<00:13,  3.33s/it]\n",
      "EP_95 | 10-fold val loss: 9.970262872943124e-05 |\n",
      "Epoch:  97%|█████████▋| 97/100 [05:36<00:09,  3.31s/it]\n",
      "EP_96 | 10-fold val loss: 9.905252504098577e-05 |\n",
      "Epoch:  98%|█████████▊| 98/100 [05:39<00:06,  3.30s/it]\n",
      "EP_97 | 10-fold val loss: 9.83993042110324e-05 |\n",
      "Epoch:  99%|█████████▉| 99/100 [05:43<00:03,  3.30s/it]\n",
      "EP_98 | 10-fold val loss: 9.774549726702674e-05 |\n",
      "Epoch: 100%|██████████| 100/100 [05:46<00:00,  3.47s/it]\n",
      "EP_99 | 10-fold val loss: 9.708258983343566e-05 |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AttractiveTrainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for classification, not better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "AttractiveNet(\n",
       "  (embedding): AttractiveEmbedding(\n",
       "    (token): TokenEmbedding(12699, 300, padding_idx=1)\n",
       "  )\n",
       "  (bigramcnn): Sequential(\n",
       "    (0): Conv1d(300, 200, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): Conv1d(200, 100, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (trigramcnn): Sequential(\n",
       "    (0): Conv1d(300, 200, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): Conv1d(200, 100, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (encoder_bigram_first): LSTM(100, 30, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "  (encoder_bigram_second): LSTM(60, 30, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "  (encoder_trigram_first): LSTM(100, 30, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "  (encoder_trigram_second): LSTM(60, 30, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "  (linear): Sequential(\n",
       "    (0): Linear(in_features=240, out_features=30, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=30, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "from attractivenet import AttractiveNet\n",
    "\n",
    "PATH = './model/CNN_LSTM_20201104-150343_0.3549.85'\n",
    "\n",
    "load_model = AttractiveNet(config).to(AttractiveData.device)\n",
    "load_model.load_state_dict(torch.load(PATH))\n",
    "load_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_attractive(sentence, category, phase):\n",
    "    indexed_sentence = [AttractiveData.TEXT.vocab.stoi[t] for t in sentence]\n",
    "    indexed_category = [AttractiveData.CATEGORIES_LABEL.vocab.stoi[category]]\n",
    "    tensor_sentence = torch.LongTensor(indexed_sentence).to(AttractiveData.device)\n",
    "    tensor_category = torch.LongTensor(indexed_category).to(AttractiveData.device)\n",
    "\n",
    "    tensor_sentence = tensor_sentence.unsqueeze(0)\n",
    "    # print(tensor_sentence.shape)\n",
    "\n",
    "    prediction = load_model(tensor_sentence, tensor_category, phase=phase)\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train mean = 3.15, test mean = 2.8\n",
    "predict_list = []\n",
    "with torch.no_grad():\n",
    "    for i, sentence in enumerate(AttractiveData.test_data):\n",
    "        prediction = predict_attractive(sentence.Headline, sentence.Category, 'test')\n",
    "        predict_list.append(prediction.item())\n",
    "        # predict_list.append(prediction.item())\n",
    "AttractiveData.df_test['Label'] = predict_list\n",
    "AttractiveData.df_test[['ID', 'Label']].to_csv(config['save_name'] + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below for statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train mean = 3.15, test mean = 2.8\n",
    "# train_list = []\n",
    "# for i, sentence in enumerate(AttractiveData.train_data):\n",
    "#     prediction = predict_attractive(sentence.Headline, sentence.Category, 'train')\n",
    "#     train_list.append(prediction.item())\n",
    "#     # train_list.append(prediction.item())\n",
    "# # print(train_list)\n",
    "# mean_squared_error(pd.read_csv('data/train.csv').sort_values(['ID']).Label.to_list(), train_list), statistics.mean(train_list), statistics.stdev(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_list[0:5], pd.read_csv('data/train.csv').sort_values(['ID']).Label.to_list()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = AttractiveData.df_train['Label'].to_list()\n",
    "# statistics.mean(a), statistics.stdev(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2.849695928296329, 0.3770257875481673)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "statistics.mean(predict_list), statistics.stdev(predict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.018452828976924473, 2.832396388579045, 0.3599597701326745)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "baseline_list = pd.read_csv('baseline.csv').sort_values(['ID']).Label.to_list()\n",
    "mean_squared_error(baseline_list, predict_list), statistics.mean(baseline_list), statistics.stdev(baseline_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.1609754968529207"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "mean_squared_error(pd.read_csv('baseline.csv').sort_values(['ID']).Label.to_list(), pd.read_csv('./predict/ensemble.csv').sort_values(['ID']).Label.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2040"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "len(AttractiveData.train_data.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_hw2",
   "language": "python",
   "name": "ds_hw2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}