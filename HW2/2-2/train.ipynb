{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common packages\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# # DL framework\n",
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "from attractivedata import AttractiveData\n",
    "from trainer import AttractiveTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
    "torch.backends.cudnn.deterministic = True  #needed\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'data/train.csv'\n",
    "val_file = 'example/val.csv'\n",
    "test_file = 'data/test.csv'\n",
    "pretrained_file = 'glove.840B.300d'\n",
    "config = {\n",
    "    'max_seq': 40,\n",
    "    'min_freq': 0,\n",
    "    'batch_size': 4,\n",
    "    'pretrained_file': pretrained_file\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AttractiveData = AttractiveData(train_file, val_file, test_file, pretrained_file, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([12699, 300])\n"
     ]
    }
   ],
   "source": [
    "config['timestr'] = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "config['save_name'] = 'CNN_LSTM'\n",
    "config['input_dim'] = len(AttractiveData.TEXT.vocab)\n",
    "config['embedding_dim'] = 300\n",
    "config['category_dim'] = len(AttractiveData.CATEGORIES_LABEL.vocab)\n",
    "config['category_embedding_dim'] = 10\n",
    "config['hidden_dim'] = 30\n",
    "config['output_dim'] = 1\n",
    "config['log_steps'] = 10\n",
    "config['epochs'] = 150\n",
    "config['lr'] = {\n",
    "    'encoder': 1e-5,\n",
    "    'embedding': 6e-6,\n",
    "    'linear': 1e-5\n",
    "}\n",
    "config['num_layers'] = 1\n",
    "config['kernel_size'] = 3\n",
    "config['dropout'] = 0.5\n",
    "config['train_len'] = AttractiveData.train_len\n",
    "config['val_len'] = AttractiveData.val_len\n",
    "config['test_len'] = AttractiveData.test_len\n",
    "\n",
    "pretrained_embeddings = AttractiveData.TEXT.vocab.vectors\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AttractiveTrainer = AttractiveTrainer(config, AttractiveData.device, AttractiveData.trainloader, AttractiveData.valloader, pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(AttractiveNet(\n",
       "   (embedding): AttractiveEmbedding(\n",
       "     (token): TokenEmbedding(12699, 300, padding_idx=1)\n",
       "   )\n",
       "   (bigramcnn): Sequential(\n",
       "     (0): Conv1d(300, 210, kernel_size=(2,), stride=(1,), padding=(1,))\n",
       "     (1): ReLU()\n",
       "     (2): Conv1d(210, 100, kernel_size=(2,), stride=(1,), padding=(1,))\n",
       "     (3): ReLU()\n",
       "     (4): Dropout(p=0.5, inplace=False)\n",
       "   )\n",
       "   (trigramcnn): Sequential(\n",
       "     (0): Conv1d(300, 210, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "     (1): ReLU()\n",
       "     (2): Conv1d(210, 100, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "     (3): ReLU()\n",
       "     (4): Dropout(p=0.5, inplace=False)\n",
       "   )\n",
       "   (encoder_bigram): LSTM(100, 30, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "   (encoder_trigram): LSTM(100, 30, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "   (linear): Sequential(\n",
       "     (0): Linear(in_features=128, out_features=30, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=30, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " 4297581,\n",
       " 4297581)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "AttractiveTrainer.model, AttractiveTrainer.config['total_params'], AttractiveTrainer.config['total_learned_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "| 17/170 [00:10<01:32,  1.65it/s]\n",
      "EP_16 | train loss: 0.5242495424607221 | val loss: 0.5579448447507971 |\n",
      "Epoch:  11%|█         | 18/170 [00:10<01:31,  1.67it/s]\n",
      "EP_17 | train loss: 0.5252090538249297 | val loss: 0.5415601858905718 |\n",
      "Epoch:  11%|█         | 19/170 [00:11<01:30,  1.67it/s]\n",
      "EP_18 | train loss: 0.5239342516543819 | val loss: 0.5267673228301254 |\n",
      "Epoch:  12%|█▏        | 20/170 [00:11<01:26,  1.73it/s]\n",
      "EP_19 | train loss: 0.5210536054536408 | val loss: 0.5181947280378902 |\n",
      "Epoch:  12%|█▏        | 21/170 [00:12<01:25,  1.73it/s]\n",
      "EP_20 | train loss: 0.5105094278559965 | val loss: 0.5142314796354256 |\n",
      "Epoch:  13%|█▎        | 22/170 [00:12<01:25,  1.73it/s]\n",
      "EP_21 | train loss: 0.5094643644258088 | val loss: 0.49964502336932165 |\n",
      "Epoch:  14%|█▎        | 23/170 [00:13<01:23,  1.77it/s]\n",
      "EP_22 | train loss: 0.5086523341197594 | val loss: 0.4898574697036369 |\n",
      "Epoch:  14%|█▍        | 24/170 [00:13<01:21,  1.80it/s]\n",
      "EP_23 | train loss: 0.5015354002223296 | val loss: 0.4822673645674014 |\n",
      "Epoch:  15%|█▍        | 25/170 [00:14<01:19,  1.83it/s]\n",
      "EP_24 | train loss: 0.49855032247655534 | val loss: 0.4718635123150021 |\n",
      "Epoch:  15%|█▌        | 26/170 [00:15<01:17,  1.86it/s]\n",
      "EP_25 | train loss: 0.4986451686597338 | val loss: 0.4619978718897876 |\n",
      "Epoch:  16%|█▌        | 27/170 [00:15<01:16,  1.86it/s]\n",
      "EP_26 | train loss: 0.4914935420541202 | val loss: 0.45430601402825 |\n",
      "Epoch:  16%|█▋        | 28/170 [00:16<01:16,  1.87it/s]\n",
      "EP_27 | train loss: 0.48329898937075744 | val loss: 0.4515889409710379 |\n",
      "Epoch:  17%|█▋        | 29/170 [00:16<01:16,  1.85it/s]\n",
      "EP_28 | train loss: 0.4818125374176923 | val loss: 0.44163966178894043 |\n",
      "Epoch:  18%|█▊        | 30/170 [00:17<01:16,  1.82it/s]\n",
      "EP_29 | train loss: 0.4804924086028454 | val loss: 0.43406907658950955 |\n",
      "Epoch:  18%|█▊        | 31/170 [00:17<01:16,  1.82it/s]\n",
      "EP_30 | train loss: 0.47704681929420023 | val loss: 0.4277472823273902 |\n",
      "Epoch:  19%|█▉        | 32/170 [00:18<01:14,  1.84it/s]\n",
      "EP_31 | train loss: 0.47352492014567055 | val loss: 0.42143924388231013 |\n",
      "Epoch:  19%|█▉        | 33/170 [00:18<01:14,  1.84it/s]\n",
      "EP_32 | train loss: 0.47110790879118675 | val loss: 0.4159066127795799 |\n",
      "Epoch:  20%|██        | 34/170 [00:19<01:14,  1.83it/s]\n",
      "EP_33 | train loss: 0.46654408165052824 | val loss: 0.40985630715594573 |\n",
      "Epoch:  21%|██        | 35/170 [00:19<01:13,  1.83it/s]\n",
      "EP_34 | train loss: 0.4681726109747793 | val loss: 0.4057925437010971 |\n",
      "Epoch:  21%|██        | 36/170 [00:20<01:13,  1.83it/s]\n",
      "EP_35 | train loss: 0.46248079935709635 | val loss: 0.399787120959338 |\n",
      "Epoch:  22%|██▏       | 37/170 [00:21<01:13,  1.82it/s]\n",
      "EP_36 | train loss: 0.4580605520921595 | val loss: 0.3949502735745673 |\n",
      "Epoch:  22%|██▏       | 38/170 [00:21<01:13,  1.79it/s]\n",
      "EP_37 | train loss: 0.4547355810801188 | val loss: 0.3886014015651217 |\n",
      "Epoch:  23%|██▎       | 39/170 [00:22<01:12,  1.80it/s]\n",
      "EP_38 | train loss: 0.4564313386000839 | val loss: 0.3871532433173236 |\n",
      "Epoch:  24%|██▎       | 40/170 [00:22<01:11,  1.83it/s]\n",
      "EP_39 | train loss: 0.45354108810424804 | val loss: 0.382370268889502 |\n",
      "Epoch:  24%|██▍       | 41/170 [00:23<01:10,  1.83it/s]\n",
      "EP_40 | train loss: 0.45012053087645887 | val loss: 0.37853891504745857 |\n",
      "Epoch:  25%|██▍       | 42/170 [00:23<01:08,  1.86it/s]\n",
      "EP_41 | train loss: 0.4515190750944848 | val loss: 0.37766935544855457 |\n",
      "Epoch:  25%|██▌       | 43/170 [00:24<01:08,  1.85it/s]\n",
      "EP_42 | train loss: 0.4395675542307835 | val loss: 0.3678763675339082 |\n",
      "Epoch:  26%|██▌       | 44/170 [00:24<01:07,  1.86it/s]\n",
      "EP_43 | train loss: 0.43626734649433807 | val loss: 0.36414435622738855 |\n",
      "Epoch:  26%|██▋       | 45/170 [00:25<01:07,  1.86it/s]\n",
      "EP_44 | train loss: 0.43918722797842585 | val loss: 0.36342505555526883 |\n",
      "Epoch:  27%|██▋       | 46/170 [00:25<01:06,  1.87it/s]\n",
      "EP_45 | train loss: 0.43564147762223787 | val loss: 0.361095864983166 |\n",
      "Epoch:  28%|██▊       | 47/170 [00:26<01:06,  1.86it/s]\n",
      "EP_46 | train loss: 0.43251039187113444 | val loss: 0.35760632478723337 |\n",
      "Epoch:  28%|██▊       | 48/170 [00:27<01:06,  1.84it/s]\n",
      "EP_47 | train loss: 0.42813567273757036 | val loss: 0.3529608308100233 |\n",
      "Epoch:  29%|██▉       | 49/170 [00:27<01:08,  1.77it/s]\n",
      "EP_48 | train loss: 0.4246273078170477 | val loss: 0.3497173391720828 |\n",
      "Epoch:  29%|██▉       | 50/170 [00:28<01:08,  1.75it/s]\n",
      "EP_49 | train loss: 0.4238683111527387 | val loss: 0.34802645532523885 |\n",
      "Epoch:  30%|███       | 51/170 [00:28<01:07,  1.75it/s]\n",
      "EP_50 | train loss: 0.4200631969115313 | val loss: 0.34543418329136044 |\n",
      "Epoch:  31%|███       | 52/170 [00:29<01:06,  1.78it/s]\n",
      "EP_51 | train loss: 0.4178593331692266 | val loss: 0.3435890119449765 |\n",
      "Epoch:  31%|███       | 53/170 [00:29<01:05,  1.78it/s]\n",
      "EP_52 | train loss: 0.4199288840387382 | val loss: 0.3439369879516901 |\n",
      "Epoch:  32%|███▏      | 54/170 [00:30<01:04,  1.79it/s]\n",
      "EP_53 | train loss: 0.4270904994478413 | val loss: 0.34889685610930127 |\n",
      "Epoch:  32%|███▏      | 55/170 [00:31<01:04,  1.79it/s]\n",
      "EP_54 | train loss: 0.41583542823791503 | val loss: 0.3398826779688106 |\n",
      "Epoch:  33%|███▎      | 56/170 [00:31<01:03,  1.80it/s]\n",
      "EP_55 | train loss: 0.4110119880414477 | val loss: 0.3366098453601201 |\n",
      "Epoch:  34%|███▎      | 57/170 [00:32<01:02,  1.82it/s]\n",
      "EP_56 | train loss: 0.41203279915977925 | val loss: 0.33669958921039805 |\n",
      "Epoch:  34%|███▍      | 58/170 [00:32<01:01,  1.82it/s]\n",
      "EP_57 | train loss: 0.40863397635665594 | val loss: 0.3344814555317748 |\n",
      "Epoch:  35%|███▍      | 59/170 [00:33<01:01,  1.82it/s]\n",
      "EP_58 | train loss: 0.40988102005977256 | val loss: 0.33552129654323354 |\n",
      "Epoch:  35%|███▌      | 60/170 [00:33<00:59,  1.84it/s]\n",
      "EP_59 | train loss: 0.4036951677471984 | val loss: 0.3314848101022197 |\n",
      "Epoch:  36%|███▌      | 61/170 [00:34<01:00,  1.81it/s]\n",
      "EP_60 | train loss: 0.4005069148306753 | val loss: 0.3288845259184931 |\n",
      "Epoch:  36%|███▋      | 62/170 [00:34<00:58,  1.84it/s]\n",
      "EP_61 | train loss: 0.39829779512741986 | val loss: 0.3282136116542068 |\n",
      "Epoch:  37%|███▋      | 63/170 [00:35<00:59,  1.80it/s]\n",
      "EP_62 | train loss: 0.3973168784496831 | val loss: 0.32671796545094134 |\n",
      "Epoch:  38%|███▊      | 64/170 [00:35<00:57,  1.84it/s]\n",
      "EP_63 | train loss: 0.3965634722335666 | val loss: 0.3262569448527168 |\n",
      "Epoch:  38%|███▊      | 65/170 [00:36<00:57,  1.84it/s]\n",
      "EP_64 | train loss: 0.390523211628783 | val loss: 0.3244454705832051 |\n",
      "Epoch:  39%|███▉      | 66/170 [00:36<00:55,  1.86it/s]\n",
      "EP_65 | train loss: 0.39184923358992035 | val loss: 0.3237689441325618 |\n",
      "Epoch:  39%|███▉      | 67/170 [00:37<00:55,  1.86it/s]\n",
      "EP_66 | train loss: 0.39664676516663794 | val loss: 0.3267926451037912 |\n",
      "Epoch:  40%|████      | 68/170 [00:38<00:55,  1.84it/s]\n",
      "EP_67 | train loss: 0.39055378857780904 | val loss: 0.3240204584949157 |\n",
      "Epoch:  41%|████      | 69/170 [00:38<00:55,  1.82it/s]\n",
      "EP_68 | train loss: 0.3849895659615012 | val loss: 0.32114022473494214 |\n",
      "Epoch:  41%|████      | 70/170 [00:39<00:54,  1.83it/s]\n",
      "EP_69 | train loss: 0.3822190541847079 | val loss: 0.31975088604525026 |\n",
      "Epoch:  42%|████▏     | 71/170 [00:39<00:54,  1.81it/s]\n",
      "EP_70 | train loss: 0.3846603515101414 | val loss: 0.32005459683782916 |\n",
      "Epoch:  42%|████▏     | 72/170 [00:40<00:54,  1.79it/s]\n",
      "EP_71 | train loss: 0.37538476270787857 | val loss: 0.3175942964997946 |\n",
      "Epoch:  43%|████▎     | 73/170 [00:40<00:54,  1.78it/s]\n",
      "EP_72 | train loss: 0.37395964183059394 | val loss: 0.31559858924033596 |\n",
      "Epoch:  44%|████▎     | 74/170 [00:41<00:55,  1.72it/s]\n",
      "EP_73 | train loss: 0.38034812749600877 | val loss: 0.318123611165028 |\n",
      "Epoch:  44%|████▍     | 75/170 [00:42<00:55,  1.71it/s]\n",
      "EP_74 | train loss: 0.3687198526719037 | val loss: 0.3133993589994954 |\n",
      "Epoch:  45%|████▍     | 76/170 [00:42<00:54,  1.72it/s]\n",
      "EP_75 | train loss: 0.37385735932518455 | val loss: 0.31479452082923814 |\n",
      "Epoch:  45%|████▌     | 77/170 [00:43<00:54,  1.72it/s]\n",
      "EP_76 | train loss: 0.3643379412445368 | val loss: 0.3112541951385199 |\n",
      "Epoch:  46%|████▌     | 78/170 [00:43<00:53,  1.73it/s]\n",
      "EP_77 | train loss: 0.3596798625646853 | val loss: 0.3101988107550378 |\n",
      "Epoch:  46%|████▋     | 79/170 [00:44<00:53,  1.71it/s]\n",
      "EP_78 | train loss: 0.3593432809792313 | val loss: 0.30889707540764527 |\n",
      "Epoch:  47%|████▋     | 80/170 [00:45<00:52,  1.71it/s]\n",
      "EP_79 | train loss: 0.3551911326015697 | val loss: 0.30788496458062936 |\n",
      "Epoch:  48%|████▊     | 81/170 [00:45<00:52,  1.71it/s]\n",
      "EP_80 | train loss: 0.3577939851611268 | val loss: 0.30663756178874596 |\n",
      "Epoch:  48%|████▊     | 82/170 [00:46<00:51,  1.72it/s]\n",
      "EP_81 | train loss: 0.3491783048592362 | val loss: 0.3060542397639331 |\n",
      "Epoch:  49%|████▉     | 83/170 [00:46<00:51,  1.70it/s]\n",
      "EP_82 | train loss: 0.35387100331923543 | val loss: 0.3036336945552452 |\n",
      "Epoch:  49%|████▉     | 84/170 [00:47<00:51,  1.68it/s]\n",
      "EP_83 | train loss: 0.3514299467498181 | val loss: 0.3017917462423736 |\n",
      "Epoch:  50%|█████     | 85/170 [00:47<00:51,  1.66it/s]\n",
      "EP_84 | train loss: 0.3412554899851481 | val loss: 0.2999502706761454 |\n",
      "Epoch:  51%|█████     | 86/170 [00:48<00:49,  1.69it/s]\n",
      "EP_85 | train loss: 0.340423799496071 | val loss: 0.2975720316171646 |\n",
      "Epoch:  51%|█████     | 87/170 [00:49<00:49,  1.67it/s]\n",
      "EP_86 | train loss: 0.3428617416643629 | val loss: 0.29765307377366457 |\n",
      "Epoch:  52%|█████▏    | 88/170 [00:49<00:49,  1.64it/s]\n",
      "EP_87 | train loss: 0.34649348212223424 | val loss: 0.2979795032856511 |\n",
      "Epoch:  52%|█████▏    | 89/170 [00:50<00:49,  1.63it/s]\n",
      "EP_88 | train loss: 0.3287737865074008 | val loss: 0.2994699010662004 |\n",
      "Epoch:  53%|█████▎    | 90/170 [00:51<00:48,  1.66it/s]\n",
      "EP_89 | train loss: 0.33312174736284744 | val loss: 0.29218491678144415 |\n",
      "Epoch:  54%|█████▎    | 91/170 [00:51<00:48,  1.61it/s]\n",
      "EP_90 | train loss: 0.32929054475298114 | val loss: 0.2894135342509139 |\n",
      "Epoch:  54%|█████▍    | 92/170 [00:52<00:47,  1.64it/s]\n",
      "EP_91 | train loss: 0.3259429609074312 | val loss: 0.2866137685144649 |\n",
      "Epoch:  55%|█████▍    | 93/170 [00:52<00:46,  1.66it/s]\n",
      "EP_92 | train loss: 0.316800820126253 | val loss: 0.28603482363270777 |\n",
      "Epoch:  55%|█████▌    | 94/170 [00:53<00:44,  1.70it/s]\n",
      "EP_93 | train loss: 0.3224893796677683 | val loss: 0.28445859048880784 |\n",
      "Epoch:  56%|█████▌    | 95/170 [00:53<00:44,  1.69it/s]\n",
      "EP_94 | train loss: 0.30974094774208816 | val loss: 0.2804306406600803 |\n",
      "Epoch:  56%|█████▋    | 96/170 [00:54<00:42,  1.73it/s]\n",
      "EP_95 | train loss: 0.3162421658927319 | val loss: 0.27952533492855 |\n",
      "Epoch:  57%|█████▋    | 97/170 [00:55<00:42,  1.70it/s]\n",
      "EP_96 | train loss: 0.3033881944768569 | val loss: 0.2752564829938552 |\n",
      "Epoch:  58%|█████▊    | 98/170 [00:55<00:42,  1.70it/s]\n",
      "EP_97 | train loss: 0.30253265348135255 | val loss: 0.27238691525132047 |\n",
      "Epoch:  58%|█████▊    | 99/170 [00:56<00:42,  1.69it/s]\n",
      "EP_98 | train loss: 0.29595680961421894 | val loss: 0.2736313074242835 |\n",
      "Epoch:  59%|█████▉    | 100/170 [00:56<00:41,  1.69it/s]\n",
      "EP_99 | train loss: 0.30644682786043953 | val loss: 0.26996757119309667 |\n",
      "Epoch:  59%|█████▉    | 101/170 [00:57<00:41,  1.68it/s]\n",
      "EP_100 | train loss: 0.2883172493354947 | val loss: 0.2650136959319021 |\n",
      "Epoch:  60%|██████    | 102/170 [00:58<00:39,  1.71it/s]\n",
      "EP_101 | train loss: 0.28769279624901567 | val loss: 0.26130284135248144 |\n",
      "Epoch:  61%|██████    | 103/170 [00:58<00:39,  1.71it/s]\n",
      "EP_102 | train loss: 0.289756315128476 | val loss: 0.26050562572245506 |\n",
      "Epoch:  61%|██████    | 104/170 [00:59<00:37,  1.74it/s]\n",
      "EP_103 | train loss: 0.2794821535839754 | val loss: 0.2546291687324935 |\n",
      "Epoch:  62%|██████▏   | 105/170 [00:59<00:37,  1.73it/s]\n",
      "EP_104 | train loss: 0.2829542087573631 | val loss: 0.25384577056940866 |\n",
      "Epoch:  62%|██████▏   | 106/170 [01:00<00:35,  1.78it/s]\n",
      "EP_105 | train loss: 0.2780766524520575 | val loss: 0.24948611358801523 |\n",
      "Epoch:  63%|██████▎   | 107/170 [01:00<00:35,  1.78it/s]\n",
      "EP_106 | train loss: 0.26639159567215864 | val loss: 0.2555534009839974 |\n",
      "Epoch:  64%|██████▎   | 108/170 [01:01<00:35,  1.76it/s]\n",
      "EP_107 | train loss: 0.27115117521846993 | val loss: 0.24353871216960982 |\n",
      "Epoch:  64%|██████▍   | 109/170 [01:02<00:34,  1.78it/s]\n",
      "EP_108 | train loss: 0.2588578684657228 | val loss: 0.23812970110014373 |\n",
      "Epoch:  65%|██████▍   | 110/170 [01:02<00:33,  1.77it/s]\n",
      "EP_109 | train loss: 0.255539478040209 | val loss: 0.23503675998425952 |\n",
      "Epoch:  65%|██████▌   | 111/170 [01:03<00:33,  1.79it/s]\n",
      "EP_110 | train loss: 0.24982319111917534 | val loss: 0.2332261911794251 |\n",
      "Epoch:  66%|██████▌   | 112/170 [01:03<00:32,  1.80it/s]\n",
      "EP_111 | train loss: 0.24686780466752895 | val loss: 0.2373037040233612 |\n",
      "Epoch:  66%|██████▋   | 113/170 [01:04<00:31,  1.79it/s]\n",
      "EP_112 | train loss: 0.2455005498493419 | val loss: 0.22450238057211333 |\n",
      "Epoch:  67%|██████▋   | 114/170 [01:04<00:30,  1.81it/s]\n",
      "EP_113 | train loss: 0.24016644697563322 | val loss: 0.22096691908789615 |\n",
      "Epoch:  68%|██████▊   | 115/170 [01:05<00:31,  1.77it/s]\n",
      "EP_114 | train loss: 0.24561406514223885 | val loss: 0.2190389373138839 |\n",
      "Epoch:  68%|██████▊   | 116/170 [01:06<00:31,  1.73it/s]\n",
      "EP_115 | train loss: 0.22926445147570443 | val loss: 0.21704543221230602 |\n",
      "Epoch:  69%|██████▉   | 117/170 [01:06<00:30,  1.73it/s]\n",
      "EP_116 | train loss: 0.23732615989797257 | val loss: 0.21149821374930589 |\n",
      "Epoch:  69%|██████▉   | 118/170 [01:07<00:29,  1.75it/s]\n",
      "EP_117 | train loss: 0.220627728396771 | val loss: 0.20948977680767283 |\n",
      "Epoch:  70%|███████   | 119/170 [01:07<00:29,  1.75it/s]\n",
      "EP_118 | train loss: 0.21543831708384495 | val loss: 0.20317971531082601 |\n",
      "Epoch:  71%|███████   | 120/170 [01:08<00:28,  1.77it/s]\n",
      "EP_119 | train loss: 0.2125059723854065 | val loss: 0.1966417452283934 |\n",
      "Epoch:  71%|███████   | 121/170 [01:08<00:27,  1.78it/s]\n",
      "EP_120 | train loss: 0.20941071358381533 | val loss: 0.19204576576457305 |\n",
      "Epoch:  72%|███████▏  | 122/170 [01:09<00:26,  1.79it/s]\n",
      "EP_121 | train loss: 0.20399426420529684 | val loss: 0.18779325134613933 |\n",
      "Epoch:  72%|███████▏  | 123/170 [01:09<00:26,  1.79it/s]\n",
      "EP_122 | train loss: 0.19777625366753224 | val loss: 0.18507659815105737 |\n",
      "Epoch:  73%|███████▎  | 124/170 [01:10<00:25,  1.81it/s]\n",
      "EP_123 | train loss: 0.20081551378848506 | val loss: 0.17948166061850154 |\n",
      "Epoch:  74%|███████▎  | 125/170 [01:11<00:25,  1.80it/s]\n",
      "EP_124 | train loss: 0.19102128136391733 | val loss: 0.18622294798785566 |\n",
      "Epoch:  74%|███████▍  | 126/170 [01:11<00:24,  1.80it/s]\n",
      "EP_125 | train loss: 0.1840959401691661 | val loss: 0.1722003092952803 |\n",
      "Epoch:  75%|███████▍  | 127/170 [01:12<00:23,  1.79it/s]\n",
      "EP_126 | train loss: 0.179918512877296 | val loss: 0.16874942797071793 |\n",
      "Epoch:  75%|███████▌  | 128/170 [01:12<00:23,  1.81it/s]\n",
      "EP_127 | train loss: 0.17517268190196916 | val loss: 0.16494555625261045 |\n",
      "Epoch:  76%|███████▌  | 129/170 [01:13<00:22,  1.79it/s]\n",
      "EP_128 | train loss: 0.1734304523935505 | val loss: 0.15664345581157535 |\n",
      "Epoch:  76%|███████▋  | 130/170 [01:13<00:22,  1.81it/s]\n",
      "EP_129 | train loss: 0.17416324568729774 | val loss: 0.17139291675651774 |\n",
      "Epoch:  77%|███████▋  | 131/170 [01:14<00:21,  1.79it/s]\n",
      "EP_130 | train loss: 0.1653193810406853 | val loss: 0.14881235448753133 |\n",
      "Epoch:  78%|███████▊  | 132/170 [01:14<00:21,  1.80it/s]\n",
      "EP_131 | train loss: 0.15753035708969715 | val loss: 0.14266045975918865 |\n",
      "Epoch:  78%|███████▊  | 133/170 [01:15<00:20,  1.79it/s]\n",
      "EP_132 | train loss: 0.16732449344560213 | val loss: 0.14257560115234524 |\n",
      "Epoch:  79%|███████▉  | 134/170 [01:16<00:20,  1.79it/s]\n",
      "EP_133 | train loss: 0.14952725744714923 | val loss: 0.14191108357672597 |\n",
      "Epoch:  79%|███████▉  | 135/170 [01:16<00:19,  1.79it/s]\n",
      "EP_134 | train loss: 0.14525928941427493 | val loss: 0.1308755874633789 |\n",
      "Epoch:  80%|████████  | 136/170 [01:17<00:18,  1.80it/s]\n",
      "EP_135 | train loss: 0.1408656861267838 | val loss: 0.13208243163192973 |\n",
      "Epoch:  81%|████████  | 137/170 [01:17<00:18,  1.79it/s]\n",
      "EP_136 | train loss: 0.1360512533608605 | val loss: 0.12315212190151215 |\n",
      "Epoch:  81%|████████  | 138/170 [01:18<00:17,  1.80it/s]\n",
      "EP_137 | train loss: 0.13651073306214576 | val loss: 0.11780189766603358 |\n",
      "Epoch:  82%|████████▏ | 139/170 [01:18<00:17,  1.79it/s]\n",
      "EP_138 | train loss: 0.14820482017947179 | val loss: 0.12262709292710997 |\n",
      "Epoch:  82%|████████▏ | 140/170 [01:19<00:16,  1.79it/s]\n",
      "EP_139 | train loss: 0.13742910576801673 | val loss: 0.11604543760711071 |\n",
      "Epoch:  83%|████████▎ | 141/170 [01:19<00:16,  1.77it/s]\n",
      "EP_140 | train loss: 0.12076569339808296 | val loss: 0.10757350395707523 |\n",
      "Epoch:  84%|████████▎ | 142/170 [01:20<00:15,  1.78it/s]\n",
      "EP_141 | train loss: 0.12489869267332787 | val loss: 0.10652354155101028 |\n",
      "Epoch:  84%|████████▍ | 143/170 [01:21<00:15,  1.78it/s]\n",
      "EP_142 | train loss: 0.13028406208636714 | val loss: 0.10807676876292509 |\n",
      "Epoch:  85%|████████▍ | 144/170 [01:21<00:14,  1.80it/s]\n",
      "EP_143 | train loss: 0.10821204395855175 | val loss: 0.10278570768879909 |\n",
      "Epoch:  85%|████████▌ | 145/170 [01:22<00:13,  1.79it/s]\n",
      "EP_144 | train loss: 0.10311778853921329 | val loss: 0.09656140062154508 |\n",
      "Epoch:  86%|████████▌ | 146/170 [01:22<00:13,  1.80it/s]\n",
      "EP_145 | train loss: 0.10273038195628746 | val loss: 0.09125463810621523 |\n",
      "Epoch:  86%|████████▋ | 147/170 [01:23<00:12,  1.79it/s]\n",
      "EP_146 | train loss: 0.09867329936401517 | val loss: 0.08732414844573713 |\n",
      "Epoch:  87%|████████▋ | 148/170 [01:23<00:12,  1.80it/s]\n",
      "EP_147 | train loss: 0.09404434766255174 | val loss: 0.09027391058557174 |\n",
      "Epoch:  88%|████████▊ | 149/170 [01:24<00:11,  1.78it/s]\n",
      "EP_148 | train loss: 0.08894008059127657 | val loss: 0.08014960440934873 |\n",
      "Epoch:  88%|████████▊ | 150/170 [01:24<00:11,  1.80it/s]\n",
      "EP_149 | train loss: 0.12435462626756406 | val loss: 0.10157633850387499 |\n",
      "Epoch:  89%|████████▉ | 151/170 [01:25<00:10,  1.79it/s]\n",
      "EP_150 | train loss: 0.08265514110817629 | val loss: 0.07566568547604131 |\n",
      "Epoch:  89%|████████▉ | 152/170 [01:26<00:10,  1.79it/s]\n",
      "EP_151 | train loss: 0.08030978645764145 | val loss: 0.07428493949712492 |\n",
      "Epoch:  90%|█████████ | 153/170 [01:26<00:09,  1.79it/s]\n",
      "EP_152 | train loss: 0.07644192219949236 | val loss: 0.07416468449667388 |\n",
      "Epoch:  91%|█████████ | 154/170 [01:27<00:08,  1.79it/s]\n",
      "EP_153 | train loss: 0.09305487280967188 | val loss: 0.07542895248123244 |\n",
      "Epoch:  91%|█████████ | 155/170 [01:27<00:08,  1.79it/s]\n",
      "EP_154 | train loss: 0.0705126083949033 | val loss: 0.06614486069655885 |\n",
      "Epoch:  92%|█████████▏| 156/170 [01:28<00:07,  1.79it/s]\n",
      "EP_155 | train loss: 0.07821401667361166 | val loss: 0.06488460974366057 |\n",
      "Epoch:  92%|█████████▏| 157/170 [01:28<00:07,  1.77it/s]\n",
      "EP_156 | train loss: 0.07727497412877925 | val loss: 0.06558062662096585 |\n",
      "Epoch:  93%|█████████▎| 158/170 [01:29<00:06,  1.78it/s]\n",
      "EP_157 | train loss: 0.06348936329869663 | val loss: 0.06074331196791986 |\n",
      "Epoch:  94%|█████████▎| 159/170 [01:30<00:06,  1.77it/s]\n",
      "EP_158 | train loss: 0.06034629853332744 | val loss: 0.05703858790152213 |\n",
      "Epoch:  94%|█████████▍| 160/170 [01:30<00:05,  1.79it/s]\n",
      "EP_159 | train loss: 0.06737290021835589 | val loss: 0.05922565338950531 |\n",
      "Epoch:  95%|█████████▍| 161/170 [01:31<00:05,  1.78it/s]\n",
      "EP_160 | train loss: 0.061017255046788385 | val loss: 0.056610432909984215 |\n",
      "Epoch:  95%|█████████▌| 162/170 [01:31<00:04,  1.79it/s]\n",
      "EP_161 | train loss: 0.05761840723308862 | val loss: 0.05130960383251602 |\n",
      "Epoch:  96%|█████████▌| 163/170 [01:32<00:03,  1.78it/s]\n",
      "EP_162 | train loss: 0.05234583557820788 | val loss: 0.049151480124861584 |\n",
      "Epoch:  96%|█████████▋| 164/170 [01:32<00:03,  1.80it/s]\n",
      "EP_163 | train loss: 0.05104139531944312 | val loss: 0.051567567329780725 |\n",
      "Epoch:  97%|█████████▋| 165/170 [01:33<00:02,  1.79it/s]\n",
      "EP_164 | train loss: 0.07441257460444581 | val loss: 0.06522904234189614 |\n",
      "Epoch:  98%|█████████▊| 166/170 [01:33<00:02,  1.79it/s]\n",
      "EP_165 | train loss: 0.04695541747644836 | val loss: 0.04854440448038718 |\n",
      "Epoch:  98%|█████████▊| 167/170 [01:34<00:01,  1.78it/s]\n",
      "EP_166 | train loss: 0.051729354759057364 | val loss: 0.04661548378712991 |\n",
      "Epoch:  99%|█████████▉| 168/170 [01:35<00:01,  1.79it/s]\n",
      "EP_167 | train loss: 0.04518736171371797 | val loss: 0.05066510757394865 |\n",
      "Epoch:  99%|█████████▉| 169/170 [01:35<00:00,  1.78it/s]\n",
      "EP_168 | train loss: 0.04183433523365095 | val loss: 0.04500973509515033 |\n",
      "Epoch: 100%|██████████| 170/170 [01:36<00:00,  1.77it/s]\n",
      "EP_169 | train loss: 0.055104615349395605 | val loss: 0.0471164405930276 |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AttractiveTrainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "AttractiveNet(\n",
       "  (embedding): AttractiveEmbedding(\n",
       "    (token): TokenEmbedding(12699, 300, padding_idx=1)\n",
       "  )\n",
       "  (bigramcnn): Sequential(\n",
       "    (0): Conv1d(300, 210, kernel_size=(2,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): Conv1d(210, 100, kernel_size=(2,), stride=(1,), padding=(1,))\n",
       "    (3): ReLU()\n",
       "    (4): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (trigramcnn): Sequential(\n",
       "    (0): Conv1d(300, 210, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): Conv1d(210, 100, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (3): ReLU()\n",
       "    (4): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (encoder_bigram): LSTM(100, 30, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (encoder_trigram): LSTM(100, 30, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (linear): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=30, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=30, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "from attractivenet import AttractiveNet\n",
    "\n",
    "PATH = './model/CNN_LSTM_20201110-124540/0.368720.74'\n",
    "\n",
    "load_model = AttractiveNet(config).to(AttractiveData.device)\n",
    "load_model.load_state_dict(torch.load(PATH))\n",
    "load_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_attractive(sentence, category, phase):\n",
    "    indexed_sentence = [AttractiveData.TEXT.vocab.stoi[t] for t in sentence]\n",
    "    indexed_category = [AttractiveData.CATEGORIES_LABEL.vocab.stoi[category]]\n",
    "    tensor_sentence = torch.LongTensor(indexed_sentence).to(AttractiveData.device)\n",
    "    tensor_category = torch.LongTensor(indexed_category).to(AttractiveData.device)\n",
    "    tensor_sentence = tensor_sentence.unsqueeze(0)\n",
    "\n",
    "    prediction = load_model(tensor_sentence, tensor_category, phase=phase)\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train mean = 3.15, test mean = 2.8\n",
    "predict_list = []\n",
    "with torch.no_grad():\n",
    "    for i, sentence in enumerate(AttractiveData.test_data):\n",
    "        prediction = predict_attractive(sentence.Headline, sentence.Category, 'test')\n",
    "        predict_list.append(prediction.item())\n",
    "        # predict_list.append(prediction.item())\n",
    "AttractiveData.df_test['Label'] = predict_list\n",
    "AttractiveData.df_test[['ID', 'Label']].to_csv(config['save_name'] + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below for statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train mean = 3.15, test mean = 2.8\n",
    "# train_list = []\n",
    "# for i, sentence in enumerate(AttractiveData.train_data):\n",
    "#     prediction = predict_attractive(sentence.Headline, sentence.Category, 'train')\n",
    "#     train_list.append(prediction.item())\n",
    "#     # train_list.append(prediction.item())\n",
    "# # print(train_list)\n",
    "# mean_squared_error(pd.read_csv('data/train.csv').sort_values(['ID']).Label.to_list(), train_list), statistics.mean(train_list), statistics.stdev(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_list[0:5], pd.read_csv('data/train.csv').sort_values(['ID']).Label.to_list()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = AttractiveData.df_train['Label'].to_list()\n",
    "# statistics.mean(a), statistics.stdev(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2.8142020345259344, 0.36798823904910916)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "statistics.mean(predict_list), statistics.stdev(predict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.0008883648376805097, 2.7876357276009043, 0.3643672504501976)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "baseline_list = pd.read_csv('baseline.csv').sort_values(['ID']).Label.to_list()\n",
    "mean_squared_error(baseline_list, predict_list), statistics.mean(baseline_list), statistics.stdev(baseline_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "mean_squared_error(pd.read_csv('baseline.csv').sort_values(['ID']).Label.to_list(), pd.read_csv('../309551062/predict/CNN_LSTM_20201109-125007_0.374958.75.csv').sort_values(['ID']).Label.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_hw2",
   "language": "python",
   "name": "ds_hw2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}