{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.12 64-bit ('DS_hw2': conda)",
   "display_name": "Python 3.6.12 64-bit ('DS_hw2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "de6ec751e8d816810380d8c8f13d270f86a313990c5d5930d253da012ff5e7fc"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common packages\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# # DL framework\n",
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "from attractivedata import AttractiveData\n",
    "from trainer import AttractiveTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "source": [
    "## Load and prepare data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'data/train.csv'\n",
    "test_file = 'data/test.csv'\n",
    "pretrained_file = 'glove.840B.300d'\n",
    "config = {\n",
    "    'max_seq': 48,\n",
    "    'min_freq': 0,\n",
    "    'batch_size': 32,\n",
    "    'pretrained_file': pretrained_file\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AttractiveData = AttractiveData(train_file, test_file, pretrained_file, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'Headline': ['sorry,', 'i', 'spent', 'it', 'on', 'myself!', 'harvey', \"nichols'\", 'hilarious', 'christmas', 'advert', 'sees', 'people', 'treating', 'themselves', 'instead', 'of', 'others'], 'Category': 'femail', 'Label': '0.18333333333333313'} {'Headline': ['three', 'police', 'officers', 'accused', 'of', 'stealing', '??', '30k', 'during', 'raid', 'on', 'criminal'], 'Category': 'news'}\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(AttractiveData.test_data):\n",
    "    if i == 3:\n",
    "        print(vars(AttractiveData.train_data[i]), vars(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "len(AttractiveData.CATEGORIES_LABEL.vocab.freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "max_len = 0\n",
    "a = AttractiveData.train_data\n",
    "for i in range(len(a)):\n",
    "    if len(a[i].Headline) >= max_len:\n",
    "        max_len = len(a[i].Headline)\n",
    "max_len"
   ]
  },
  {
   "source": [
    "## Start to train"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([12699, 300])\n"
     ]
    }
   ],
   "source": [
    "config['timestr'] = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "config['save_name'] = 'LSTM'\n",
    "config['input_dim'] = len(AttractiveData.TEXT.vocab)\n",
    "config['embedding_dim'] = 300\n",
    "config['category_dim'] = len(AttractiveData.CATEGORIES_LABEL.vocab)\n",
    "config['category_embedding_dim'] = 16\n",
    "config['hidden_dim'] = 30\n",
    "config['output_dim'] = 1\n",
    "config['log_steps'] = 10\n",
    "config['epochs'] = 50\n",
    "config['lr'] = {\n",
    "    'encoder': 1e-4,\n",
    "    'embedding': 1e-5,\n",
    "    'linear': 1e-4\n",
    "}\n",
    "config['num_layers'] = 2\n",
    "config['kernel_size'] = 3\n",
    "config['dropout'] = 0.0\n",
    "\n",
    "pretrained_embeddings = AttractiveData.TEXT.vocab.vectors\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "AttractiveTrainer = AttractiveTrainer(config, AttractiveData.device, AttractiveData.trainloader, pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(AttractiveNet(\n",
       "   (embedding): AttractiveEmbedding(\n",
       "     (token): TokenEmbedding(12699, 300, padding_idx=1)\n",
       "   )\n",
       "   (encoder): LSTM(300, 30, bidirectional=True)\n",
       "   (linear): Sequential(\n",
       "     (0): Linear(in_features=120, out_features=30, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=30, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " 3893041,\n",
       " 83341)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "AttractiveTrainer.model, AttractiveTrainer.config['total_params'], AttractiveTrainer.config['total_learned_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'max_seq': 48,\n",
       " 'min_freq': 0,\n",
       " 'batch_size': 32,\n",
       " 'pretrained_file': 'glove.840B.300d',\n",
       " 'timestr': '20201101-001046',\n",
       " 'save_name': 'LSTM',\n",
       " 'input_dim': 12699,\n",
       " 'embedding_dim': 300,\n",
       " 'category_dim': 18,\n",
       " 'category_embedding_dim': 16,\n",
       " 'hidden_dim': 30,\n",
       " 'output_dim': 1,\n",
       " 'log_steps': 10,\n",
       " 'epochs': 50,\n",
       " 'lr': {'encoder': 0.0001, 'embedding': 1e-05, 'linear': 0.0001},\n",
       " 'num_layers': 1,\n",
       " 'kernel_size': 3,\n",
       " 'dropout': 0.0,\n",
       " 'total_params': 3893041,\n",
       " 'total_learned_params': 83341}"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch:   2%|▏         | 1/50 [00:00<00:19,  2.54it/s]\n",
      "EP_train | avg_loss: 0.560712771024555 |\n",
      "Epoch:   4%|▍         | 2/50 [00:00<00:18,  2.64it/s]\n",
      "EP_train | avg_loss: 0.5604117317125201 |\n",
      "Epoch:   6%|▌         | 3/50 [00:01<00:17,  2.71it/s]\n",
      "EP_train | avg_loss: 0.559755980502814 |\n",
      "Epoch:   8%|▊         | 4/50 [00:01<00:16,  2.77it/s]\n",
      "EP_train | avg_loss: 0.559899449814111 |\n",
      "Epoch:  10%|█         | 5/50 [00:01<00:16,  2.80it/s]\n",
      "EP_train | avg_loss: 0.5591805945150554 |\n",
      "Epoch:  12%|█▏        | 6/50 [00:02<00:15,  2.84it/s]\n",
      "EP_train | avg_loss: 0.559345934074372 |\n",
      "Epoch:  14%|█▍        | 7/50 [00:02<00:15,  2.73it/s]\n",
      "EP_train | avg_loss: 0.5591005655005574 |\n",
      "Epoch:  16%|█▌        | 8/50 [00:02<00:15,  2.76it/s]\n",
      "EP_train | avg_loss: 0.5584538909606636 |\n",
      "Epoch:  18%|█▊        | 9/50 [00:03<00:15,  2.70it/s]\n",
      "EP_train | avg_loss: 0.5573523449711502 |\n",
      "Epoch:  20%|██        | 10/50 [00:03<00:14,  2.70it/s]\n",
      "EP_train | avg_loss: 0.557686950545758 |\n",
      "Epoch:  22%|██▏       | 11/50 [00:04<00:14,  2.66it/s]\n",
      "EP_train | avg_loss: 0.5583644560538232 |\n",
      "Epoch:  24%|██▍       | 12/50 [00:04<00:14,  2.69it/s]\n",
      "EP_train | avg_loss: 0.5577991278842092 |\n",
      "Epoch:  26%|██▌       | 13/50 [00:04<00:13,  2.70it/s]\n",
      "EP_train | avg_loss: 0.5563506460748613 |\n",
      "Epoch:  28%|██▊       | 14/50 [00:05<00:13,  2.70it/s]\n",
      "EP_train | avg_loss: 0.5573632214218378 |\n",
      "Epoch:  30%|███       | 15/50 [00:05<00:12,  2.71it/s]\n",
      "EP_train | avg_loss: 0.5567344431765378 |\n",
      "Epoch:  32%|███▏      | 16/50 [00:05<00:12,  2.72it/s]\n",
      "EP_train | avg_loss: 0.5562581494450569 |\n",
      "Epoch:  34%|███▍      | 17/50 [00:06<00:12,  2.74it/s]\n",
      "EP_train | avg_loss: 0.5566597948782146 |\n",
      "Epoch:  36%|███▌      | 18/50 [00:06<00:11,  2.77it/s]\n",
      "EP_train | avg_loss: 0.5556745217181742 |\n",
      "Epoch:  38%|███▊      | 19/50 [00:06<00:11,  2.78it/s]\n",
      "EP_train | avg_loss: 0.5559265939518809 |\n",
      "Epoch:  40%|████      | 20/50 [00:07<00:10,  2.80it/s]\n",
      "EP_train | avg_loss: 0.5566801629029214 |\n",
      "Epoch:  42%|████▏     | 21/50 [00:07<00:10,  2.81it/s]\n",
      "EP_train | avg_loss: 0.55511833447963 |\n",
      "Epoch:  44%|████▍     | 22/50 [00:07<00:09,  2.81it/s]\n",
      "EP_train | avg_loss: 0.556185545399785 |\n",
      "Epoch:  46%|████▌     | 23/50 [00:08<00:09,  2.77it/s]\n",
      "EP_train | avg_loss: 0.5558008551597595 |\n",
      "Epoch:  48%|████▊     | 24/50 [00:08<00:09,  2.85it/s]\n",
      "EP_train | avg_loss: 0.5544860432855785 |\n",
      "Epoch:  50%|█████     | 25/50 [00:09<00:08,  2.91it/s]\n",
      "EP_train | avg_loss: 0.5562664386816323 |\n",
      "Epoch:  52%|█████▏    | 26/50 [00:09<00:08,  2.90it/s]\n",
      "EP_train | avg_loss: 0.5543067017570138 |\n",
      "Epoch:  54%|█████▍    | 27/50 [00:09<00:08,  2.84it/s]\n",
      "EP_train | avg_loss: 0.5536602754145861 |\n",
      "Epoch:  56%|█████▌    | 28/50 [00:10<00:07,  2.80it/s]\n",
      "EP_train | avg_loss: 0.5541710290126503 |\n",
      "Epoch:  58%|█████▊    | 29/50 [00:10<00:07,  2.85it/s]\n",
      "EP_train | avg_loss: 0.5535452123731375 |\n",
      "Epoch:  60%|██████    | 30/50 [00:10<00:07,  2.80it/s]\n",
      "EP_train | avg_loss: 0.5529940892010927 |\n",
      "Epoch:  62%|██████▏   | 31/50 [00:11<00:06,  2.78it/s]\n",
      "EP_train | avg_loss: 0.553749795537442 |\n",
      "Epoch:  64%|██████▍   | 32/50 [00:11<00:06,  2.79it/s]\n",
      "EP_train | avg_loss: 0.5540801668539643 |\n",
      "Epoch:  66%|██████▌   | 33/50 [00:11<00:06,  2.79it/s]\n",
      "EP_train | avg_loss: 0.5522875650785863 |\n",
      "Epoch:  68%|██████▊   | 34/50 [00:12<00:05,  2.82it/s]\n",
      "EP_train | avg_loss: 0.5527926711365581 |\n",
      "Epoch:  70%|███████   | 35/50 [00:12<00:05,  2.82it/s]\n",
      "EP_train | avg_loss: 0.5530637144111097 |\n",
      "Epoch:  72%|███████▏  | 36/50 [00:12<00:04,  2.87it/s]\n",
      "EP_train | avg_loss: 0.5528485304675996 |\n",
      "Epoch:  74%|███████▍  | 37/50 [00:13<00:04,  2.85it/s]\n",
      "EP_train | avg_loss: 0.5516877504996955 |\n",
      "Epoch:  76%|███████▌  | 38/50 [00:13<00:04,  2.84it/s]\n",
      "EP_train | avg_loss: 0.5512371766380966 |\n",
      "Epoch:  78%|███████▊  | 39/50 [00:13<00:03,  2.86it/s]\n",
      "EP_train | avg_loss: 0.5514296675100923 |\n",
      "Epoch:  80%|████████  | 40/50 [00:14<00:03,  2.87it/s]\n",
      "EP_train | avg_loss: 0.5512931286357343 |\n",
      "Epoch:  82%|████████▏ | 41/50 [00:14<00:03,  2.87it/s]\n",
      "EP_train | avg_loss: 0.5514479745179415 |\n",
      "Epoch:  84%|████████▍ | 42/50 [00:14<00:02,  2.90it/s]\n",
      "EP_train | avg_loss: 0.5513633517548442 |\n",
      "Epoch:  86%|████████▌ | 43/50 [00:15<00:02,  2.92it/s]\n",
      "EP_train | avg_loss: 0.5522133652120829 |\n",
      "Epoch:  88%|████████▊ | 44/50 [00:15<00:02,  2.94it/s]\n",
      "EP_train | avg_loss: 0.551616526208818 |\n",
      "Epoch:  90%|█████████ | 45/50 [00:15<00:01,  2.95it/s]\n",
      "EP_train | avg_loss: 0.5501502091065049 |\n",
      "Epoch:  92%|█████████▏| 46/50 [00:16<00:01,  2.96it/s]\n",
      "EP_train | avg_loss: 0.5514016347005963 |\n",
      "Epoch:  94%|█████████▍| 47/50 [00:16<00:01,  2.98it/s]\n",
      "EP_train | avg_loss: 0.5506838983856142 |\n",
      "Epoch:  96%|█████████▌| 48/50 [00:17<00:00,  2.92it/s]\n",
      "EP_train | avg_loss: 0.5496144844219089 |\n",
      "Epoch:  98%|█████████▊| 49/50 [00:17<00:00,  2.84it/s]\n",
      "EP_train | avg_loss: 0.5501307928934693 |\n",
      "Epoch: 100%|██████████| 50/50 [00:17<00:00,  2.82it/s]\n",
      "EP_train | avg_loss: 0.5494997526984662 |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AttractiveTrainer.train()"
   ]
  },
  {
   "source": [
    "## for classification, not better"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5601443355119825"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# from sklearn.metrics import mean_squared_error\n",
    "# a = AttractiveTrainer.train_predict\n",
    "# AttractiveData.LABEL.vocab.itos[int(a[0])], AttractiveTrainer.train_true[0]\n",
    "# correct = 0\n",
    "# pred_list = []\n",
    "# true_list = []\n",
    "# for i in range(len(a)):\n",
    "#     pred = AttractiveData.LABEL.vocab.itos[int(a[i])]\n",
    "#     pred_list.append(float(pred))\n",
    "#     true = AttractiveData.LABEL.vocab.itos[int(AttractiveTrainer.train_true[i])]\n",
    "#     true_list.append(float(true))\n",
    "# mean_squared_error(true_list, pred_list)\n",
    "# # true_list"
   ]
  },
  {
   "source": [
    "## Below is testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "AttractiveNet(\n",
       "  (embedding): AttractiveEmbedding(\n",
       "    (token): TokenEmbedding(12699, 300, padding_idx=1)\n",
       "  )\n",
       "  (encoder): LSTM(300, 30, num_layers=2, bidirectional=True)\n",
       "  (linear): Sequential(\n",
       "    (0): Linear(in_features=120, out_features=30, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=30, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "from attractivenet import AttractiveNet\n",
    "PATH = './model/LSTM_20201101-000626_0.5353.50'\n",
    "# load_model = TransformerModel(config).to(AttractiveData.device)\n",
    "load_model = AttractiveNet(config).to(AttractiveData.device)\n",
    "load_model.load_state_dict(torch.load(PATH))\n",
    "load_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_attractive(sentence, category):\n",
    "    if len(sentence) < config['max_size']:\n",
    "        sentence += ['0'] * (config['max_size'] - len(sentence))\n",
    "    else:\n",
    "        sentence = sentence[:config['max_size']]\n",
    "\n",
    "    indexed_sentence = [AttractiveData.TEXT.vocab.stoi[t] for t in sentence]\n",
    "    indexed_category = [AttractiveData.CATEGORIES_LABEL.vocab.stoi[category]]\n",
    "    tensor_sentence = torch.LongTensor(indexed_sentence).to(AttractiveData.device)\n",
    "    tensor_category = torch.LongTensor(indexed_category).to(AttractiveData.device)\n",
    "\n",
    "    tensor_sentence = tensor_sentence.unsqueeze(1)\n",
    "\n",
    "    prediction = load_model(tensor_sentence, tensor_category)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train mean = 3.15, test mean = 2.8\n",
    "predict_list = []\n",
    "with torch.no_grad():\n",
    "    for i, sentence in enumerate(AttractiveData.test_data):\n",
    "        # print(i)\n",
    "        # print(sentence.Headline)\n",
    "        prediction = predict_attractive(sentence.Headline, sentence.Category)\n",
    "        predict_list.append(prediction.item() + 2.8)\n",
    "        # predict_list.append(prediction.item())\n",
    "AttractiveData.df_test['Label'] = predict_list\n",
    "AttractiveData.df_test[['ID', 'Label']].to_csv(config['save_name'] + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_category = list(AttractiveData.CATEGORIES_LABEL.vocab.freqs)\n",
    "# test_category = list(AttractiveData.df_test['Category'].value_counts().keys())\n",
    "# for each_test in test_category:\n",
    "#     if each_test not in train_category:\n",
    "#         print(each_test)\n",
    "# print()\n",
    "# for each_train in train_category:\n",
    "#     if each_train not in test_category:\n",
    "#         print(each_train)"
   ]
  },
  {
   "source": [
    "## Below just for fun guess"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.5347357640381534, 3.212537875745044, 0.009437110101923997)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# train mean = 3.15, test mean = 2.8\n",
    "train_list = []\n",
    "for i, sentence in enumerate(AttractiveData.train_data):\n",
    "    prediction = predict_attractive(sentence.Headline, sentence.Category)\n",
    "    train_list.append(prediction.item() + 3.15)\n",
    "    # train_list.append(prediction.item())\n",
    "mean_squared_error(pd.read_csv('data/train.csv').sort_values(['ID']).Label.to_list(), train_list), statistics.mean(train_list), statistics.stdev(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.0004084967320261136, 0.7295015193216009)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "a = AttractiveData.df_train['Label'].to_list()\n",
    "statistics.mean(a), statistics.stdev(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2.8603686607225347, 0.0100302224724178)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "statistics.mean(predict_list), statistics.stdev(predict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.10607111434061829, 2.7156126782757597, 0.29355123275379763)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "baseline_list = pd.read_csv('baseline.csv').sort_values(['ID']).Label.to_list()\n",
    "mean_squared_error(baseline_list, predict_list), statistics.mean(baseline_list), statistics.stdev(baseline_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2.8167915543795683, 0.14611407210842048)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# LSTM my best\n",
    "# statistics.mean(predict_list), statistics.stdev(predict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.1347375515605904, 2.8379913731293533, 0.1903582104725371)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "a = pd.read_csv('LSTM_base.csv').Label.to_list()\n",
    "mean_squared_error(baseline_list, a), statistics.mean(a), statistics.stdev(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}